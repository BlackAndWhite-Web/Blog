# 音视频导出
- 将preview渲染的内容合成视频导出; 渲染的画面导出 + 播放的声音导出 = 合成视频；
- 渲染的画面导出：DOM录制
- 播放的声音导出：录制&合成(navigator.mediaDevices.getUserMedia())

## 开源工具调研
- RecordRTC: WebRTC JavaScript库音频+视频+屏幕+画布(2D+3D动画)record;
- FFmpeg：多媒体框架，用于处理音视频等流媒体处理工具，包括解码，编码，转码等。采用c/c++来编写，基于webAssembly（一种可以使用非js编程语言编程，并能在浏览器上运行的方案），可以在浏览器端运行。

## 部分web api简介
- MediaDevices API提供访问连接媒体输入的设备，如照相机和麦克风，以及屏幕共享等。它可以使你取得任何硬件资源的媒体数据。
- MediaStream API是WebRTC中描述媒体数据流。
- MediaRecorder是MediaStream Recording API提供的用来实现媒体元素录制的接口。


## 渲染画面—— DOM录制
- DOM快照：将DOM操作，以快照的形式存储下来；常用于bug复现; => 将一张张快照当成序列帧播放；
  html2canvas + canvas2image(canvas.toDataURL)
- MediaStream API提供的用来进行media element(video, audio, canvas)录制的接口 ---- MediaRecorder => 将canvas转换成stream,在对stream进行录制。
  stream = canvas.captureStream(frameRate);
  recorder = new MediaRecorder(stream, { mimeType: 'video/webm' });
  canvas.captureStream(), 可以将canvas转成webm或者mp4视频，本质上就是个录屏功能；
  目前采用可以使用canvas.captureStream()实现；来实现画面的合成
  ```js
      const exportVideo = () => {
        const canvas = document.querySelector('canvas');
        const stream = canvas.captureStream();
        recorder = new MediaRecorder(stream, { mimeType: 'video/webm' });
        const data = [];
        recorder.ondataavailable = function (event) {
          if (event.data && event.data.size) {
            data.push(event.data);
          }
        };
        recorder.onstop = () => {
          const url = URL.createObjectURL(new Blob(data, { type: 'video/webm' }));
        };
      }
  ```
- RecordRTC: 本质也是将canvas转换成stream,在对stream进行录制。

## 播放声音——录制&合成
- FFmpeg：将多媒体资源解码后合成在编码成统一输出；
- RecordRTC: 采用录制系统声音或者外界声音。
- 基于MediaStream，利用media element原生自带的，captureStream, 然后获取到getTracks轨道信息，通过MediaRecorder来进行录制，最后进行合并处理等；
- navigator.mediaDevices.getUserMedia({audio: true, video: true}), 获取到麦克风摄像头等录制的声音。
- navigator.mediaDevices.enumerateDevices()  + getDisplayMedia，获取到输入和输出的媒体设备，然后通过屏幕捕捉，获取到系统声音;


### ffmpeg问题
- 问题1：基于ffmpeg转码一个视频&音频耗时需要很久；
- 问题2: ffmpeg基于c/c++；运行在浏览器上面有些兼容性和各种运行报错需要排查；
- 问题3: ffmpeg一些语法配置需要学习成本

### RecordRTC问题
- 问题1: 只能收集麦克风声音；
- 问题2: 系统声音收集，虽有插件支持，但是插件和网页是隔离的，并不能拿到插件上面的录制音轨信息


### 基于MediaStream，录制音频
- 需要计算各类资源和处理做拼接，还有空白段等的处理等；处理起来比较麻烦。

## 音视频轨道的融合合成
- 参考RecordRTC实现，将视频轨道和音频轨道做融合，输出最终的视频blob;
```js
var finalStream = new MediaStream();
// 音频轨道
getTracks(audioStream, 'audio').forEach(function(track) {
    finalStream.addTrack(track);
});
// 视频轨道
getTracks(canvasStream, 'video').forEach(function(track) {
    finalStream.addTrack(track);
});

var recorder = RecordRTC(finalStream, {
    type: 'video'
});

recorder.startRecording();
```

## 项目应用
- preview组件提供一个导出的功能;
- 导出过程中，全局loading;
- 融合完的数据在需要上传到obs；
    blob转成file;
    file上传obs;
- 上传完的url做本地下载？存储到后端，接口层面支持？

## webm转mp4
- 浏览器转视频格式 ffmpeg => createFFmpeg

### 业务需要修改的点
- 座舱视角不是基于canvas, 需要调整；如果基于canvas做录制，需要调整；
- 录屏阶段需要完整的一次播放逻辑
- 依赖obs做上传，需要有上传lib的支持；

### 问题
- 导出视频清晰度问题：
  设置videoBitsPerSecond（设置视频码率：单位时间内数据流量；码率越大文件越大，视频也约清晰）;
```js
    recorder = g.RecordRTC(finalStream, {
        type: 'video/mp4',
        mimeType: 'video/mp4',
        videoBitsPerSecond : 8500000, // 视频码率
    });
```
- 目前webrtc录制导出的视频都是webm格式，只有safari是可以支持mp4格式；如何将webm -> mp4;




## todo
- 进一步了解相关音视频相关知识等；